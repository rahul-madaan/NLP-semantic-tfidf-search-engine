{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Project with ANN implementation",
      "provenance": [],
      "collapsed_sections": [
        "ACbjNjyO4f_8",
        "g6pXBVxsVUbm"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACbjNjyO4f_8"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCM50vaM4jiK"
      },
      "source": [
        "# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qOVy-_vmuUP"
      },
      "source": [
        "# Semantic Search with Approximate Nearest Neighbors and Text Embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T4d77AJaKte"
      },
      "source": [
        "\n",
        "\n",
        "The steps are:\n",
        "1. Download sample data.\n",
        "2. Generate embeddings for the data using a TF-Hub model\n",
        "3. Build an ANN index for the embeddings\n",
        "4. Use the index for similarity matching\n",
        "\n",
        "We use [Apache Beam](https://beam.apache.org/documentation/programming-guide/) to generate the embeddings from the TF-Hub model. \n",
        "\n",
        "We also use Spotify's [ANNOY](https://github.com/spotify/annoy) library to build the approximate nearest neighbours index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whMRj9qeqed4"
      },
      "source": [
        "Install the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmXkLPoaqS--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805e7754-4fe5-4e4a-b355-91b2a038a96b"
      },
      "source": [
        "!pip install apache_beam\n",
        "!pip install 'scikit_learn~=0.23.0'  # For gaussian_random_matrix.\n",
        "!pip install annoy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apache_beam\n",
            "  Downloading apache_beam-2.34.0-cp37-cp37m-manylinux2010_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (2.8.2)\n",
            "Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (4.1.3)\n",
            "Requirement already satisfied: numpy<1.21.0,>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions<4,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.10.0.2)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.3.0)\n",
            "Collecting fastavro<2,>=0.21.4\n",
            "  Downloading fastavro-1.4.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 32.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.7)\n",
            "Collecting requests<3.0.0,>=2.24.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 760 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (2018.9)\n",
            "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.42.0)\n",
            "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (0.17.4)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 59.9 MB/s \n",
            "\u001b[?25hCollecting avro-python3!=1.9.2,<1.10.0,>=1.8.1\n",
            "  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.12.1)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB)\n",
            "\u001b[K     |████████████████████████████████| 249 kB 50.3 MB/s \n",
            "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 35.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.17.3)\n",
            "Requirement already satisfied: pyarrow<6.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<2,>=1.29.0->apache_beam) (1.15.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (0.6.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache_beam) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache_beam) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache_beam) (4.8)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot<2,>=1.2.0->apache_beam) (3.0.6)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.0.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.10)\n",
            "Building wheels for collected packages: avro-python3, dill, future\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=41d40c56e83bdb47533ab476724f6510c60e90ec39e8cd53e72128fdad5b98c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=2f3caa71a1e3340ac34604cc550bcbec67159702bceaca23a6678a6084cf96c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=319abd77b2cfa3afee06ec644f6abfd069fd1ed27f8a375cae854142d73a5280\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built avro-python3 dill future\n",
            "Installing collected packages: requests, orjson, hdfs, future, fastavro, dill, avro-python3, apache-beam\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed apache-beam-2.34.0 avro-python3-1.9.2.1 dill-0.3.1.1 fastavro-1.4.7 future-0.18.2 hdfs-2.6.0 orjson-3.6.4 requests-2.26.0\n",
            "Collecting scikit_learn~=0.23.0\n",
            "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit_learn~=0.23.0) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn~=0.23.0) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit_learn~=0.23.0) (1.19.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn~=0.23.0) (3.0.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\u001b[0m\n",
            "Successfully installed scikit-learn-0.23.2\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.0.tar.gz (646 kB)\n",
            "\u001b[K     |████████████████████████████████| 646 kB 6.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391672 sha256=75aca12bf957c305832e2ce2887cce8cac2b014de05b32ad78544deb49ac2b2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-vBZiCCqld0"
      },
      "source": [
        "Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NTYbdWcseuK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "416a1d02-92aa-403d-c08a-e6bdd18c6fdb"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "from collections import namedtuple\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "!pip install apache_beam\n",
        "import apache_beam as beam\n",
        "from apache_beam.transforms import util\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import annoy\n",
        "from sklearn.random_projection import gaussian_random_matrix"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: apache_beam in /usr/local/lib/python3.7/dist-packages (2.34.0)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (0.3.1.1)\n",
            "Requirement already satisfied: future<1.0.0,>=0.18.2 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (0.18.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (2.8.2)\n",
            "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (0.17.4)\n",
            "Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.9.2.1)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.7)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.3.0)\n",
            "Requirement already satisfied: fastavro<2,>=0.21.4 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.4.7)\n",
            "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.17.3)\n",
            "Requirement already satisfied: orjson<4.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.6.4)\n",
            "Requirement already satisfied: typing-extensions<4,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.10.0.2)\n",
            "Requirement already satisfied: numpy<1.21.0,>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.19.5)\n",
            "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.42.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (2.26.0)\n",
            "Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (4.1.3)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (2.6.0)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.12.1)\n",
            "Requirement already satisfied: pyarrow<6.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.0.0)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (2018.9)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<2,>=1.29.0->apache_beam) (1.15.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (0.6.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache_beam) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache_beam) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache_beam) (0.2.8)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot<2,>=1.2.0->apache_beam) (3.0.6)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx0SZa6-7b-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "908993f9-710d-418f-a97c-71b9c00afcfb"
      },
      "source": [
        "print('TF version: {}'.format(tf.__version__))\n",
        "print('TF-Hub version: {}'.format(hub.__version__))\n",
        "print('Apache Beam version: {}'.format(beam.__version__))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version: 2.7.0\n",
            "TF-Hub version: 0.12.0\n",
            "Apache Beam version: 2.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Imq876rLWx"
      },
      "source": [
        "## 1. Download Sample Data\n",
        "\n",
        "Indian news headlines dataset from kaggle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpF57n8e5C9D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35548cf3-f00e-40ea-a16b-c59b56ed173c"
      },
      "source": [
        "! pip install kaggle\n",
        "!rm -r ~/.kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download therohk/india-headlines-news-dataset\n",
        "! unzip india-headlines-news-dataset"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.26.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.0.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "rm: cannot remove '/root/.kaggle': No such file or directory\n",
            "Downloading india-headlines-news-dataset.zip to /content\n",
            " 81% 65.0M/80.4M [00:00<00:00, 66.1MB/s]\n",
            "100% 80.4M/80.4M [00:01<00:00, 82.1MB/s]\n",
            "Archive:  india-headlines-news-dataset.zip\n",
            "  inflating: india-news-headlines.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ1GEnrghvcC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9be0a622-b157-4134-df6f-d716b8b1fa14"
      },
      "source": [
        "import pandas as pd\n",
        "news_df = pd.read_csv(\"india-news-headlines.csv\", nrows = 60000)\n",
        "print(\"Dataframe shape:\", news_df.shape)\n",
        "pd.set_option('max_columns', None)\n",
        "news_df.drop(\"headline_category\",axis=1,inplace=True)\n",
        "print(news_df.head())\n",
        "news_df.sort_values(\"headline_text\", inplace = True)\n",
        "news_df.drop_duplicates(subset =\"headline_text\",keep = False, inplace = True)\n",
        "news_df.to_csv('raw.tsv', sep = '\\t', index=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe shape: (60000, 3)\n",
            "   publish_date                                      headline_text\n",
            "0      20010102  Status quo will not be disturbed at Ayodhya; s...\n",
            "1      20010102                Fissures in Hurriyat over Pak visit\n",
            "2      20010102              America's unwanted heading for India?\n",
            "3      20010102                 For bigwigs; it is destination Goa\n",
            "4      20010102               Extra buses to clear tourist traffic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Reeoc9z0zTxJ"
      },
      "source": [
        "For simplicity, we only keep the headline text and remove the publication date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INPWa4upv_yJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11cedf05-9b7f-4a5f-a055-a60254818851"
      },
      "source": [
        "!rm -r corpus\n",
        "!mkdir corpus\n",
        "\n",
        "with open('corpus/text.txt', 'w') as out_file:\n",
        "  with open('raw.tsv', 'r') as in_file:\n",
        "    for line in in_file:\n",
        "      headline = line.split('\\t')[1].strip().strip('\"')\n",
        "      out_file.write(headline+\"\\n\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'corpus': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-oedX40z6o2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0749a3ff-dd74-469c-fe5b-6b5f9454d714"
      },
      "source": [
        "!tail corpus/text.txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iAppliance to rewrite Net access\n",
            "iBackOffice plans huge call centre base in Karnataka\n",
            "iCMG releases world's first CORBA Server\n",
            "iFlex in tie-up with Trintech\n",
            "iFlex readies bill presentment product\n",
            "neoIT MarketMaker to power B2B marketplace\n",
            "omancing the thorn cactii\n",
            "reak waters soon for Ullal: MLA\n",
            "riter bags Marathi award\n",
            "what the SC order really means\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AngMtH50jNb"
      },
      "source": [
        "## 2. Generate Embeddings for the Data.\n",
        "\n",
        "In this tutorial, we use the [Neural Network Language Model (NNLM)](https://tfhub.dev/google/nnlm-en-dim128/2) to generate embeddings for the headline data. The sentence embeddings can then be easily used to compute sentence level meaning similarity. We run the embedding generation process using Apache Beam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_DvXnDB1pEX"
      },
      "source": [
        "### Embedding extraction method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL7OEY1E0A35"
      },
      "source": [
        "embed_fn = None\n",
        "\n",
        "def generate_embeddings(text, model_url, random_projection_matrix=None):\n",
        "  # Beam will run this function in different processes that need to\n",
        "  # import hub and load embed_fn (if not previously loaded)\n",
        "  global embed_fn\n",
        "  if embed_fn is None:\n",
        "    embed_fn = hub.load(model_url)\n",
        "  embedding = embed_fn(text).numpy()\n",
        "  if random_projection_matrix is not None:\n",
        "    embedding = embedding.dot(random_projection_matrix)\n",
        "  return text, embedding\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6pXBVxsVUbm"
      },
      "source": [
        "### Convert to tf.Example method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMjqjWZNVVzd"
      },
      "source": [
        "def to_tf_example(entries):\n",
        "  examples = []\n",
        "\n",
        "  text_list, embedding_list = entries\n",
        "  for i in range(len(text_list)):\n",
        "    text = text_list[i]\n",
        "    embedding = embedding_list[i]\n",
        "\n",
        "    features = {\n",
        "        'text': tf.train.Feature(\n",
        "            bytes_list=tf.train.BytesList(value=[text.encode('utf-8')])),\n",
        "        'embedding': tf.train.Feature(\n",
        "            float_list=tf.train.FloatList(value=embedding.tolist()))\n",
        "    }\n",
        "  \n",
        "    example = tf.train.Example(\n",
        "        features=tf.train.Features(\n",
        "            feature=features)).SerializeToString(deterministic=True)\n",
        "  \n",
        "    examples.append(example)\n",
        "  \n",
        "  return examples"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDiV4uQCVYGH"
      },
      "source": [
        "### Beam pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCGUIB172m2G"
      },
      "source": [
        "def run_hub2emb(args):\n",
        "  '''Runs the embedding generation pipeline'''\n",
        "\n",
        "  options = beam.options.pipeline_options.PipelineOptions(**args)\n",
        "  args = namedtuple(\"options\", args.keys())(*args.values())\n",
        "\n",
        "  with beam.Pipeline(args.runner, options=options) as pipeline:\n",
        "    (\n",
        "        pipeline\n",
        "        | 'Read sentences from files' >> beam.io.ReadFromText(\n",
        "            file_pattern=args.data_dir)\n",
        "        | 'Batch elements' >> util.BatchElements(\n",
        "            min_batch_size=args.batch_size, max_batch_size=args.batch_size)\n",
        "        | 'Generate embeddings' >> beam.Map(\n",
        "            generate_embeddings, args.model_url, args.random_projection_matrix)\n",
        "        | 'Encode to tf example' >> beam.FlatMap(to_tf_example)\n",
        "        | 'Write to TFRecords files' >> beam.io.WriteToTFRecord(\n",
        "            file_path_prefix='{}/emb'.format(args.output_dir),\n",
        "            file_name_suffix='.tfrecords')\n",
        "    )"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlbQdiYNVvne"
      },
      "source": [
        "### Generating Projection Weight Matrix\n",
        "Enbeddings\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yw1xgtNVv52"
      },
      "source": [
        "#testing the paralell processing pipeline\n",
        "\n",
        "def generate_random_projection_weights(original_dim, projected_dim):\n",
        "  random_projection_matrix = None\n",
        "  random_projection_matrix = gaussian_random_matrix(\n",
        "      n_components=projected_dim, n_features=original_dim).T\n",
        "  print(\"A Gaussian random weight matrix was creates with shape of {}\".format(random_projection_matrix.shape))\n",
        "  print('Storing random projection matrix to disk...')\n",
        "  with open('random_projection_matrix', 'wb') as handle:\n",
        "    pickle.dump(random_projection_matrix, \n",
        "                handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        \n",
        "  return random_projection_matrix\n",
        "model_url = 'https://tfhub.dev/google/nnlm-en-dim128/2'\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJZUfT3NE7kj"
      },
      "source": [
        "### Set dimension\n",
        "Tradeoff between time to find embedding and accuracy of the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77-Cow7uE74T"
      },
      "source": [
        "projected_dim = 64"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On-MbzD922kb"
      },
      "source": [
        "### Run pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3I1Wv4i21yY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd3b99ca-1d94-4052-a0e7-8dce4ab2643a"
      },
      "source": [
        "import tempfile\n",
        "\n",
        "output_dir = tempfile.mkdtemp()\n",
        "original_dim = hub.load(model_url)(['']).shape[1]\n",
        "random_projection_matrix = None\n",
        "\n",
        "if projected_dim:\n",
        "  random_projection_matrix = generate_random_projection_weights(\n",
        "      original_dim, projected_dim)\n",
        "\n",
        "args = {\n",
        "    'job_name': 'hub2emb-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S')),\n",
        "    'runner': 'DirectRunner',\n",
        "    'batch_size': 1024,\n",
        "    'data_dir': 'corpus/*.txt',\n",
        "    'output_dir': output_dir,\n",
        "    'model_url': model_url,\n",
        "    'random_projection_matrix': random_projection_matrix,\n",
        "}\n",
        "\n",
        "print(\"Pipeline args are set.\")\n",
        "args"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Gaussian random weight matrix was creates with shape of (128, 64)\n",
            "Storing random projection matrix to disk...\n",
            "Pipeline args are set.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:86: FutureWarning: Function gaussian_random_matrix is deprecated; gaussian_random_matrix is deprecated in 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 1024,\n",
              " 'data_dir': 'corpus/*.txt',\n",
              " 'job_name': 'hub2emb-211205-012529',\n",
              " 'model_url': 'https://tfhub.dev/google/nnlm-en-dim128/2',\n",
              " 'output_dir': '/tmp/tmpfq1zn7tb',\n",
              " 'random_projection_matrix': array([[-2.24099586e-01, -2.06534132e-01,  7.33447876e-02, ...,\n",
              "         -8.90196586e-02, -2.91140163e-02, -1.30105420e-02],\n",
              "        [-8.59390127e-02,  1.28023437e-01, -1.86185313e-01, ...,\n",
              "          1.90947418e-01, -1.78020521e-01,  4.01730520e-02],\n",
              "        [ 4.79080752e-02,  3.92872251e-02, -1.49889939e-01, ...,\n",
              "          2.24436412e-01, -7.03432666e-02, -1.08381454e-01],\n",
              "        ...,\n",
              "        [-4.05605790e-02,  1.59242704e-01,  2.31003034e-01, ...,\n",
              "         -1.34397045e-01, -6.02687845e-03,  1.32325604e-01],\n",
              "        [ 6.77362094e-02,  4.05236309e-02, -1.47368423e-01, ...,\n",
              "         -7.53350368e-02, -5.33326883e-02, -2.73748982e-04],\n",
              "        [ 2.30124394e-01,  1.95459228e-01, -2.79128170e-01, ...,\n",
              "          2.54372855e-02,  2.27407061e-02, -1.74208985e-01]]),\n",
              " 'runner': 'DirectRunner'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS9obmeP4ZOA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "943bc48e-8c28-490b-912a-e774897e3188"
      },
      "source": [
        "print(\"Running pipeline...\")\n",
        "%time run_hub2emb(args)\n",
        "print(\"Pipeline is done.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running pipeline...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-86957179-d1e9-40bd-9006-11992601e96f.json']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'batch_size': 1024, 'data_dir': 'corpus/*.txt', 'output_dir': '/tmp/tmpfq1zn7tb', 'model_url': 'https://tfhub.dev/google/nnlm-en-dim128/2', 'random_projection_matrix': array([[-2.24099586e-01, -2.06534132e-01,  7.33447876e-02, ...,\n",
            "        -8.90196586e-02, -2.91140163e-02, -1.30105420e-02],\n",
            "       [-8.59390127e-02,  1.28023437e-01, -1.86185313e-01, ...,\n",
            "         1.90947418e-01, -1.78020521e-01,  4.01730520e-02],\n",
            "       [ 4.79080752e-02,  3.92872251e-02, -1.49889939e-01, ...,\n",
            "         2.24436412e-01, -7.03432666e-02, -1.08381454e-01],\n",
            "       ...,\n",
            "       [-4.05605790e-02,  1.59242704e-01,  2.31003034e-01, ...,\n",
            "        -1.34397045e-01, -6.02687845e-03,  1.32325604e-01],\n",
            "       [ 6.77362094e-02,  4.05236309e-02, -1.47368423e-01, ...,\n",
            "        -7.53350368e-02, -5.33326883e-02, -2.73748982e-04],\n",
            "       [ 2.30124394e-01,  1.95459228e-01, -2.79128170e-01, ...,\n",
            "         2.54372855e-02,  2.27407061e-02, -1.74208985e-01]])}\n",
            "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
            "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 8.72 s, sys: 5.83 s, total: 14.5 s\n",
            "Wall time: 8.62 s\n",
            "Pipeline is done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAwOo7gQWvVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8966a84f-ddba-44e6-efb4-31db632af757"
      },
      "source": [
        "!ls {output_dir}"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emb-00000-of-00001.tfrecords\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVnee4e6U90u"
      },
      "source": [
        "Read some of the generated embeddings..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K7pGXlXOj1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aee7eb35-3439-4ee0-ef87-7dd45044d955"
      },
      "source": [
        "embed_file = os.path.join(output_dir, 'emb-00000-of-00001.tfrecords')\n",
        "sample = 5\n",
        "\n",
        "# Create a description of the features.\n",
        "feature_description = {\n",
        "    'text': tf.io.FixedLenFeature([], tf.string),\n",
        "    'embedding': tf.io.FixedLenFeature([projected_dim], tf.float32)\n",
        "}\n",
        "\n",
        "def _parse_example(example):\n",
        "  # Parse the input `tf.Example` proto using the dictionary above.\n",
        "  return tf.io.parse_single_example(example, feature_description)\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(embed_file)\n",
        "for record in dataset.take(sample).map(_parse_example):\n",
        "  print(\"{}: {}\".format(record['text'].numpy().decode('utf-8'), record['embedding'].numpy()[:10]))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "headline_text: [ 0.0355889   0.20153058  0.14327948  0.00831385  0.13658473  0.04474842\n",
            "  0.4164145   0.06946948 -0.15532742 -0.02356505]\n",
            "#Defending the Nation: [-3.0419663e-01 -1.4821230e-02 -1.1612993e-01  1.1138162e-01\n",
            " -3.3939795e-03  6.3432746e-02  1.5327128e-04  4.9223311e-02\n",
            " -2.0585656e-01  1.4699967e-01]\n",
            "#LNN Helpline not much of a help: [-0.20051254  0.02992163  0.06441832  0.02852312  0.19194146  0.1005758\n",
            " -0.00320513  0.28817672  0.07050508 -0.01515995]\n",
            "$10 mn for Clinton memoirs: [ 0.0130038   0.05794297 -0.1563591  -0.06837318 -0.11934026 -0.02319421\n",
            " -0.1556919  -0.37758648 -0.04424743 -0.14084326]\n",
            "$100 bn bail-out package in offing: [ 0.12070765 -0.10276997  0.08971539  0.16733016 -0.09254742  0.01975778\n",
            " -0.02603568 -0.22195488 -0.09979504 -0.10005516]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agGoaMSgY8wN"
      },
      "source": [
        "## 3. Build the ANN Index for the Embeddings\n",
        "\n",
        "[ANNOY](https://github.com/spotify/annoy) (Approximate Nearest Neighbors Oh Yeah) is a C++ library with Python bindings to search for points in space that are close to a given query point. It also creates large read-only file-based data structures that are mapped into memory. It is built and used by [Spotify](https://www.spotify.com) for music recommendations. If you are interested you can play along with other alternatives to ANNOY such as [NGT](https://github.com/yahoojapan/NGT), [FAISS](https://github.com/facebookresearch/faiss), etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcPDspU3WjgH"
      },
      "source": [
        "def build_index(embedding_files_pattern, index_filename, vector_length, \n",
        "    metric='angular', num_trees=100):\n",
        "  '''Builds an ANNOY index'''\n",
        "\n",
        "  annoy_index = annoy.AnnoyIndex(vector_length, metric=metric)\n",
        "  # Mapping between the item and its identifier in the index\n",
        "  mapping = {}\n",
        "\n",
        "  embed_files = tf.io.gfile.glob(embedding_files_pattern)\n",
        "  num_files = len(embed_files)\n",
        "  print('Found {} embedding file(s).'.format(num_files))\n",
        "\n",
        "  item_counter = 0\n",
        "  for i, embed_file in enumerate(embed_files):\n",
        "    print('Loading embeddings in file {} of {}...'.format(i+1, num_files))\n",
        "    dataset = tf.data.TFRecordDataset(embed_file)\n",
        "    for record in dataset.map(_parse_example):\n",
        "      text = record['text'].numpy().decode(\"utf-8\")\n",
        "      embedding = record['embedding'].numpy()\n",
        "      mapping[item_counter] = text\n",
        "      annoy_index.add_item(item_counter, embedding)\n",
        "      item_counter += 1\n",
        "      if item_counter % 100000 == 0:\n",
        "        print('{} items loaded to the index'.format(item_counter))\n",
        "\n",
        "  print('A total of {} items added to the index'.format(item_counter))\n",
        "\n",
        "  print('Building the index with {} trees...'.format(num_trees))\n",
        "  annoy_index.build(n_trees=num_trees)\n",
        "  print('Index is successfully built.')\n",
        "  \n",
        "  print('Saving index to disk...')\n",
        "  annoy_index.save(index_filename)\n",
        "  print('Index is saved to disk.')\n",
        "  print(\"Index file size: {} GB\".format(\n",
        "    round(os.path.getsize(index_filename) / float(1024 ** 3), 2)))\n",
        "  annoy_index.unload()\n",
        "\n",
        "  print('Saving mapping to disk...')\n",
        "  with open(index_filename + '.mapping', 'wb') as handle:\n",
        "    pickle.dump(mapping, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  print('Mapping is saved to disk.')\n",
        "  print(\"Mapping file size: {} MB\".format(\n",
        "    round(os.path.getsize(index_filename + '.mapping') / float(1024 ** 2), 2)))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgyOQhUq6FNE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfe14229-d2e7-48e9-9274-f563557cb96f"
      },
      "source": [
        "embedding_files = \"{}/emb-*.tfrecords\".format(output_dir)\n",
        "embedding_dimension = projected_dim\n",
        "index_filename = \"index\"\n",
        "\n",
        "!rm {index_filename}\n",
        "!rm {index_filename}.mapping\n",
        "\n",
        "%time build_index(embedding_files, index_filename, embedding_dimension)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'index': No such file or directory\n",
            "rm: cannot remove 'index.mapping': No such file or directory\n",
            "Found 1 embedding file(s).\n",
            "Loading embeddings in file 1 of 1...\n",
            "A total of 53809 items added to the index\n",
            "Building the index with 100 trees...\n",
            "Index is successfully built.\n",
            "Saving index to disk...\n",
            "Index is saved to disk.\n",
            "Index file size: 0.07 GB\n",
            "Saving mapping to disk...\n",
            "Mapping is saved to disk.\n",
            "Mapping file size: 2.18 MB\n",
            "CPU times: user 31 s, sys: 2.08 s, total: 33.1 s\n",
            "Wall time: 25.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic31Tm5cgAd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2a9d57-75c3-4b0e-cef8-146ef50813d2"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus\t       india-headlines-news-dataset.zip  random_projection_matrix\n",
            "index\t       india-news-headlines.csv\t\t raw.tsv\n",
            "index.mapping  kaggle.json\t\t\t sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maGxDl8ufP-p"
      },
      "source": [
        "## 4. Use the Index for Similarity Matching\n",
        "Now we can use the ANN index to find news headlines that are semantically close to an input query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dIs8W78fYPp"
      },
      "source": [
        "### Load the index and the mapping files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlTTrbQHayvb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce2e2b83-89ab-4e51-87bd-0444199aa108"
      },
      "source": [
        "index = annoy.AnnoyIndex(embedding_dimension)\n",
        "index.load(index_filename, prefault=True)\n",
        "print('Annoy index is loaded.')\n",
        "with open(index_filename + '.mapping', 'rb') as handle:\n",
        "  mapping = pickle.load(handle)\n",
        "print('Mapping file is loaded.')\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annoy index is loaded.\n",
            "Mapping file is loaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6liFMSUh08J"
      },
      "source": [
        "### Similarity matching method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUxjTag8hc16"
      },
      "source": [
        "def find_similar_items(embedding, num_matches=5):\n",
        "  '''Finds similar items to a given embedding in the ANN index'''\n",
        "  ids = index.get_nns_by_vector(\n",
        "  embedding, num_matches, search_k=-1, include_distances=False)\n",
        "  items = [mapping[i] for i in ids]\n",
        "  return items"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjerNpmZja0A"
      },
      "source": [
        "### Extract embedding from a given query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0IIXzfBjZ19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "623a4d15-14bb-42ee-deba-09a36b201d48"
      },
      "source": [
        "# Load the TF-Hub model\n",
        "print(\"Loading the TF-Hub model...\")\n",
        "%time embed_fn = hub.load(model_url)\n",
        "print(\"TF-Hub model is loaded.\")\n",
        "\n",
        "random_projection_matrix = None\n",
        "if os.path.exists('random_projection_matrix'):\n",
        "  print(\"Loading random projection matrix...\")\n",
        "  with open('random_projection_matrix', 'rb') as handle:\n",
        "    random_projection_matrix = pickle.load(handle)\n",
        "  print('random projection matrix is loaded.')\n",
        "\n",
        "def extract_embeddings(query):\n",
        "  '''Generates the embedding for the query'''\n",
        "  query_embedding =  embed_fn([query])[0].numpy()\n",
        "  if random_projection_matrix is not None:\n",
        "    query_embedding = query_embedding.dot(random_projection_matrix)\n",
        "  return query_embedding\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the TF-Hub model...\n",
            "CPU times: user 577 ms, sys: 294 ms, total: 872 ms\n",
            "Wall time: 878 ms\n",
            "TF-Hub model is loaded.\n",
            "Loading random projection matrix...\n",
            "random projection matrix is loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCoCNROujEIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94932f2c-147e-42d4-c0d5-a2222444ab90"
      },
      "source": [
        "extract_embeddings(\"Hello Machine Learning!\")[:10] #test query string"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.14128624, -0.17463988,  0.09110139, -0.12706835, -0.16837585,\n",
              "       -0.03135261,  0.07596967,  0.09212895, -0.05135348, -0.12892351])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koINo8Du--8C"
      },
      "source": [
        "### Enter a query to find the most similar items"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC0uLjvfk5nB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5718187-39f2-4991-9f2f-9435aec4d437"
      },
      "source": [
        "query = \"army of Britian to start helicopter crash probe\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"Generating embedding for the query...\")\n",
        "%time query_embedding = extract_embeddings(query)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Finding relevant items in the index...\")\n",
        "%time items = find_similar_items(query_embedding, 20)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Results:\")\n",
        "print(\"=========\")\n",
        "for item in items:\n",
        "  print(item)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embedding for the query...\n",
            "CPU times: user 5.41 ms, sys: 74 µs, total: 5.49 ms\n",
            "Wall time: 5.37 ms\n",
            "\n",
            "Finding relevant items in the index...\n",
            "CPU times: user 2.7 ms, sys: 941 µs, total: 3.64 ms\n",
            "Wall time: 2.9 ms\n",
            "\n",
            "Results:\n",
            "=========\n",
            "British forces to begin copter crash investigation\n",
            "Police to buy unmanned plane to spy on naxalites\n",
            "Spate of burglaries: Police team sent to Mumbai\n",
            "Army job hopeful killed in stampede\n",
            "Aziz to visit US to firm up financial package\n",
            "Plea for CoD probe into boat fire\n",
            "Anti sabotage jeeps for Punjab police\n",
            "Case against firm for misuse of machinery\n",
            "DIG to probe into mysterious killing of trader\n",
            "Joint panel to probe airport incident\n",
            "Separate economic offences arm of CBI\n",
            "Survivor of flyover crash questioned\n",
            "Oppn fire failed to singe government\n",
            "US victim of own deeds\n",
            "Police seize documents from ex-MD of Tata Finance\n",
            "Concern over transfer of Kandla land to state govt\n",
            "SC orders probe into dump of hazardous waste\n",
            "Murder: Cops leave in search of duo\n",
            "Trade bodies to oppose transfer of Nerul cop\n",
            "Two held for trying to sell govt jeep\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WenHcaJ-3wcu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "909a4eb0-4729-4e68-8cd2-c8f83f3e7f21"
      },
      "source": [
        "# to keep notebook up even if network goes down after model runs.\n",
        "import time\n",
        "for i in range(10000):\n",
        "  time.sleep(10)\n",
        "  print(i)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-58256365b117>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}